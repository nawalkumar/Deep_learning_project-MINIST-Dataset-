steps implemented in this projects:
step 1:
Modify the MLP from the previous question to use different activation functions (ReLU,
Sigmoid, Tanh) in the hidden layers. Evaluate and compare the performance of each
model on the same dataset (MNIST).


step 2:
Explore the effect of different learning rates and optimization algorithms (SGD, Adam,
RMSprop) on the training performance of an MLP. Use a dataset like CIFAR-10 for
image classification. Comment on the generalization gap between the training and test
accuracy.


step 3:
Implement a Multi-Layer Perceptron (MLP) to classify handwritten digits from the
MNIST dataset. However, restrict the implementation to not use any deep learning
library such as TensorFlow or PyTorch, except for data loading. Specifically, you can use
libraries or utilities to load and preprocess the MNIST dataset, but all aspects of
building and training the neural network should be implemented without relying on
external deep learning libraries. Experiment with different architectures by varying the
number of hidden layers and neurons to observe their effects on classification
performance.


step 4:
Implement an MLP using a neural network library (such as TensorFlow or PyTorch) to
classify handwritten digits from the MNIST dataset. Experiment with different
numbers of hidden layers and neurons.
